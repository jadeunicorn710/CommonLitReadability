{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_experimentation_yunlin.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yi1OxA6UeFOB"},"source":["# Torch the DL Project\n","\n","To train these models please configure the experiments utilizing config files in the /configs directory of this repository.\n","\n","Acknowledgements: This notebook is derived from Pre-trained Roberta Solution in Pytorch <https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch>"]},{"cell_type":"code","metadata":{"id":"GjlfbwgJm7kA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627136084689,"user_tz":240,"elapsed":153,"user":{"displayName":"Yunlin Qi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-aKZVJ1prt3w6IZCyj5ulz60dvuBwEfir7h5F=s64","userId":"04948370213367288353"}},"outputId":"7d08dbc8-320e-45bd-e56e-9b88a0046d7d"},"source":["# Mount your google drive if you want\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LXCUF3knZEZ","executionInfo":{"status":"ok","timestamp":1627136091481,"user_tz":240,"elapsed":6632,"user":{"displayName":"Yunlin Qi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-aKZVJ1prt3w6IZCyj5ulz60dvuBwEfir7h5F=s64","userId":"04948370213367288353"}},"outputId":"516be7b5-c665-4fc1-e87d-7e8112a54495"},"source":["# Install some libraries in your environment\n","!pip install transformers\n","!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MHgmPmQInKz0"},"source":["import os\n","import math\n","import random\n","import time\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold\n","\n","import gc\n","gc.enable()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LC_dN8fpfpCQ"},"source":["# Class & Function Definitions"]},{"cell_type":"code","metadata":{"id":"PXitnqJDnbpR"},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVmnShmanUjT"},"source":["def get_data(data_path='drive/MyDrive/ColabNotebooks/7643project/Data'):\n","  train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n","  # Remove incomplete entries if any.\n","  train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n","                  inplace=True)\n","  train_df.reset_index(drop=True, inplace=True)\n","\n","  test_df = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n","  submission_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n","  return train_df, test_df, submission_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D81-VWpEopP5"},"source":["class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False, max_len=256):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only\n","        self.text = df.excerpt.tolist()\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n","    \n","        self.encoded = tokenizer.batch_encode_plus(\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = max_len,\n","            truncation = True,\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index):        \n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return (input_ids, attention_mask)            \n","        else:\n","            target = self.target[index]\n","            return (input_ids, attention_mask, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-_iMK-CpaKD"},"source":["class LitModel(nn.Module):\n","    def __init__(self, model_path): #ROBERTA_PATH\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(model_path)\n","        config.update({\"output_hidden_states\":True, \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7})                       \n","        \n","        self.roberta = AutoModel.from_pretrained(model_path, config=config)  \n","        \n","        #TODO make these configurable\n","        self.attention = nn.Sequential(            \n","            nn.Linear(768, 1024),            \n","            nn.Tanh(),                       \n","            nn.Linear(1024, 1),\n","            nn.Softmax(dim=1)\n","        )        \n","\n","        self.regressor = nn.Sequential(                        \n","            nn.Linear(768, 1)                        \n","        )\n","        self.regressor2 = nn.Sequential(                        \n","            nn.Linear(768 * 2, 1)                        \n","        )\n","        self.regressor4 = nn.Sequential(                        \n","            nn.Linear(768 * 4, 1)                        \n","        )\n","\n","        # some additional layers\n","        self.cnn1 = nn.Conv1d(768, 256, kernel_size=2, padding=1)\n","        self.cnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)\n","        \n","        self.relu = nn.Sequential(\n","            nn.ReLU()\n","        )\n","        \n","\n","    def forward(self, input_ids, attention_mask):\n","        roberta_output = self.roberta(input_ids=input_ids,\n","                                      attention_mask=attention_mask)    \n","\n","\n","        # Original model configurations #\n","\n","        # There are a total of 13 layers of hidden states.\n","        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n","        # We take the hidden states from the last Roberta layer.\n","        last_layer_hidden_states = roberta_output.hidden_states[-1]\n","\n","        # The number of cells is MAX_LEN.\n","        # The size of the hidden state of each cell is 768 (for roberta-base).\n","        # In order to condense hidden states of all cells to a context vector,\n","        # we compute a weighted average of the hidden states of all cells.\n","        # We compute the weight of each cell, using the attention neural network.\n","        weights = self.attention(last_layer_hidden_states)\n","                \n","        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n","        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n","        # Now we compute context_vector as the weighted average.\n","        # context_vector.shape is BATCH_SIZE x 768\n","        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n","        \n","        # Now we reduce the context vector to the prediction score.\n","        return self.regressor(context_vector)\n","\n","\n","\n","        # # MEAN POOLING #\n","        # last_hidden_state = roberta_output[0]\n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        # sum_mask = input_mask_expanded.sum(1)\n","        # sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        # mean_embeddings = sum_embeddings / sum_mask\n","        # logits = self.regressor(mean_embeddings)\n","        # return logits\n","\n","\n","\n","        # # MAX POOLING #\n","        # last_hidden_state = roberta_output[0]  \n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","        # max_embeddings = torch.max(last_hidden_state, 1)[0]\n","        # logits = self.regressor(max_embeddings)\n","        # return logits\n","\n","\n","\n","        # # MEAN-MAX POOLING #\n","        # last_hidden_state = roberta_output[0]\n","        # mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n","        # max_pooling_embeddings = torch.max(last_hidden_state, 1)[0]\n","        # mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n","        # logits = self.regressor2(mean_max_embeddings)\n","        # return logits\n","\n","\n","\n","        # # CONV-1D POOLING #\n","        # last_hidden_state = last_hidden_state.permute(0, 2, 1)\n","        # cnn_embeddings = self.relu(self.cnn1(last_hidden_state))\n","        # cnn_embeddings = self.cnn2(cnn_embeddings)\n","        # logits, _ = torch.max(cnn_embeddings, 2)\n","        # return logits\n","\n","\n","\n","        # # CONCATENATE POOLING #\n","        # all_hidden_states = torch.stack(roberta_output[2])\n","        # concatenate_pooling = torch.cat(\n","        #     (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n","        # )\n","        # concatenate_pooling = concatenate_pooling[:, 0]\n","\n","        # logits = self.regressor4(concatenate_pooling) # regression head\n","        # return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT21oHcMphlg"},"source":["def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval()            \n","    mse_sum = 0\n","\n","    with torch.no_grad():\n","        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)                        \n","            target = target.to(DEVICE)           \n","            \n","            pred = model(input_ids, attention_mask)                       \n","\n","            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n","                \n","\n","    return mse_sum / len(data_loader.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLnFrV1Lpm4k"},"source":["def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval()\n","\n","    result = np.zeros(len(data_loader.dataset))    \n","    index = 0\n","    \n","    with torch.no_grad():\n","        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","                        \n","            pred = model(input_ids, attention_mask)                        \n","\n","            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n","            index += pred.shape[0]\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXeGq5fzpoG-"},"source":["def train(model, model_path, train_loader, val_loader,\n","          optimizer, scheduler=None, num_epochs=3, val_schedule=None):    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = val_schedule[0][1]    \n","\n","    start = time.time()\n","\n","    for epoch in range(num_epochs):                           \n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)            \n","            target = target.to(DEVICE)                        \n","\n","            optimizer.zero_grad()\n","            \n","            model.train()\n","\n","            pred = model(input_ids, attention_mask)\n","                                                        \n","            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n","                        \n","            mse.backward()\n","\n","            optimizer.step()\n","            if scheduler:\n","                scheduler.step()\n","            \n","            if step >= last_eval_step + eval_period:\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start\n","                num_steps = step - last_eval_step\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step\n","                \n","                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n","                      f\"val_rmse: {val_rmse:0.4}\")\n","\n","                for rmse, period in val_schedule:\n","                    if val_rmse >= rmse:\n","                        eval_period = period\n","                        break                               \n","                \n","                if not best_val_rmse or val_rmse < best_val_rmse:                    \n","                    best_val_rmse = val_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path)\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n","                          f\"(from epoch {best_epoch})\")                                    \n","                    \n","                start = time.time()\n","                                            \n","            step += 1\n","                        \n","    \n","    return best_val_rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OfaBT7npsbU"},"source":["def create_optimizer(model):\n","    named_parameters = list(model.named_parameters())    \n","    \n","    roberta_parameters = named_parameters[:197]    \n","    attention_parameters = named_parameters[199:203]\n","    regressor_parameters = named_parameters[203:]\n","        \n","    attention_group = [params for (name, params) in attention_parameters]\n","    regressor_group = [params for (name, params) in regressor_parameters]\n","\n","    parameters = []\n","    parameters.append({\"params\": attention_group})\n","    parameters.append({\"params\": regressor_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters):\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 2e-5\n","\n","        if layer_num >= 69:        \n","            lr = 5e-5\n","\n","        if layer_num >= 133:\n","            lr = 1e-4\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8ndAlyzCRIR"},"source":["# Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9lpsFenzpP19","executionInfo":{"status":"ok","timestamp":1627138782717,"user_tz":240,"elapsed":2691110,"user":{"displayName":"Yunlin Qi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-aKZVJ1prt3w6IZCyj5ulz60dvuBwEfir7h5F=s64","userId":"04948370213367288353"}},"outputId":"c233c23d-a636-4918-e0ef-e08c8ef3aaf7"},"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","config_path = 'drive/MyDrive/ColabNotebooks/7643project/configs/roberta-base.json'\n","with open(config_path) as f:\n","  config = json.load(f)\n","\n","training_config = config[\"training\"]\n","dataset_config = config[\"dataset\"]\n","model_config = config[\"model\"]\n","evaluation_config = config[\"evaluation\"]\n","\n","gc.collect()\n","tokenizer = AutoTokenizer.from_pretrained(model_config[\"base_model\"])\n","list_val_rmse = []\n","\n","kfold = KFold(n_splits=training_config[\"num_folds\"], random_state=training_config[\"seed\"], shuffle=True)\n","train_df, test_df, sample_submission = get_data(dataset_config[\"data_path\"])\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)): \n","    num_folds = training_config[\"num_folds\"]\n","    print(f\"\\nFold {fold + 1}/{num_folds}\")\n","    model_path = f\"model_{fold + 1}.pth\"\n","        \n","    set_random_seed(training_config[\"seed\"] + fold)\n","    \n","    train_dataset = LitDataset(train_df.loc[train_indices])    \n","    val_dataset = LitDataset(train_df.loc[val_indices])    \n","        \n","    train_loader = DataLoader(train_dataset, batch_size=dataset_config[\"batch_size\"],\n","                              drop_last=True, shuffle=True, num_workers=dataset_config[\"num_workers\"])    \n","    val_loader = DataLoader(val_dataset, batch_size=dataset_config[\"batch_size\"],\n","                            drop_last=False, shuffle=False, num_workers=dataset_config[\"num_workers\"])    \n","        \n","    set_random_seed(training_config[\"seed\"] + fold)    \n","    \n","    model = LitModel(model_config[\"base_model\"]).to(DEVICE)\n","    \n","    optimizer = create_optimizer(model)                        \n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_training_steps=training_config[\"num_epochs\"] * len(train_loader),\n","        num_warmup_steps=50)    \n","    \n","    list_val_rmse.append(train(model, model_path, train_loader,\n","                               val_loader, optimizer, scheduler=scheduler,\n","                               num_epochs=training_config[\"num_epochs\"], \n","                               val_schedule=training_config[\"val_schedule\"]))\n","\n","    del model\n","    gc.collect()\n","    \n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Fold 1/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 7.18 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 0.9055\n","New best_val_rmse: 0.9055\n","\n","16 steps took 6.52 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.7035\n","New best_val_rmse: 0.7035\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.6519\n","New best_val_rmse: 0.6519\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.613\n","New best_val_rmse: 0.613\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.5872\n","New best_val_rmse: 0.5872\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 96 val_rmse: 0.6914\n","Still best_val_rmse: 0.5872 (from epoch 0)\n","\n","16 steps took 6.53 seconds\n","Epoch: 0 batch_num: 112 val_rmse: 0.7822\n","Still best_val_rmse: 0.5872 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 128 val_rmse: 0.6247\n","Still best_val_rmse: 0.5872 (from epoch 0)\n","\n","16 steps took 6.82 seconds\n","Epoch: 1 batch_num: 3 val_rmse: 0.6231\n","Still best_val_rmse: 0.5872 (from epoch 0)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 19 val_rmse: 0.5494\n","New best_val_rmse: 0.5494\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 35 val_rmse: 0.5545\n","Still best_val_rmse: 0.5494 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 51 val_rmse: 0.4973\n","New best_val_rmse: 0.4973\n","\n","8 steps took 3.26 seconds\n","Epoch: 1 batch_num: 59 val_rmse: 0.4965\n","New best_val_rmse: 0.4965\n","\n","8 steps took 3.27 seconds\n","Epoch: 1 batch_num: 67 val_rmse: 0.521\n","Still best_val_rmse: 0.4965 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 83 val_rmse: 0.5025\n","Still best_val_rmse: 0.4965 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 99 val_rmse: 0.4937\n","New best_val_rmse: 0.4937\n","\n","8 steps took 3.27 seconds\n","Epoch: 1 batch_num: 107 val_rmse: 0.5195\n","Still best_val_rmse: 0.4937 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 123 val_rmse: 0.4907\n","New best_val_rmse: 0.4907\n","\n","8 steps took 3.27 seconds\n","Epoch: 1 batch_num: 131 val_rmse: 0.497\n","Still best_val_rmse: 0.4907 (from epoch 1)\n","\n","8 steps took 3.26 seconds\n","Epoch: 1 batch_num: 139 val_rmse: 0.5208\n","Still best_val_rmse: 0.4907 (from epoch 1)\n","\n","16 steps took 6.81 seconds\n","Epoch: 2 batch_num: 14 val_rmse: 0.4878\n","New best_val_rmse: 0.4878\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 18 val_rmse: 0.5307\n","Still best_val_rmse: 0.4878 (from epoch 2)\n","\n","16 steps took 6.51 seconds\n","Epoch: 2 batch_num: 34 val_rmse: 0.4899\n","Still best_val_rmse: 0.4878 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 38 val_rmse: 0.5074\n","Still best_val_rmse: 0.4878 (from epoch 2)\n","\n","16 steps took 6.55 seconds\n","Epoch: 2 batch_num: 54 val_rmse: 0.4861\n","New best_val_rmse: 0.4861\n","\n","4 steps took 1.65 seconds\n","Epoch: 2 batch_num: 58 val_rmse: 0.4981\n","Still best_val_rmse: 0.4861 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 66 val_rmse: 0.4821\n","New best_val_rmse: 0.4821\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 70 val_rmse: 0.483\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 74 val_rmse: 0.483\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 78 val_rmse: 0.4882\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 82 val_rmse: 0.4938\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","8 steps took 3.26 seconds\n","Epoch: 2 batch_num: 90 val_rmse: 0.4882\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 94 val_rmse: 0.4846\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 98 val_rmse: 0.4841\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 102 val_rmse: 0.4841\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 106 val_rmse: 0.4848\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 110 val_rmse: 0.4862\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 114 val_rmse: 0.4868\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 118 val_rmse: 0.4871\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 122 val_rmse: 0.4869\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 126 val_rmse: 0.4867\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 130 val_rmse: 0.4867\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 134 val_rmse: 0.4866\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 138 val_rmse: 0.4866\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","Performance estimates:\n","[0.4820976694639964]\n","Mean: 0.4820976694639964\n","\n","Fold 2/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 7.16 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 1.13\n","New best_val_rmse: 1.13\n","\n","16 steps took 6.51 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.9548\n","New best_val_rmse: 0.9548\n","\n","16 steps took 6.51 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.684\n","New best_val_rmse: 0.684\n","\n","16 steps took 6.53 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.6209\n","New best_val_rmse: 0.6209\n","\n","16 steps took 6.51 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.5579\n","New best_val_rmse: 0.5579\n","\n","16 steps took 6.51 seconds\n","Epoch: 0 batch_num: 96 val_rmse: 0.5453\n","New best_val_rmse: 0.5453\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 112 val_rmse: 0.538\n","New best_val_rmse: 0.538\n","\n","16 steps took 6.51 seconds\n","Epoch: 0 batch_num: 128 val_rmse: 0.5332\n","New best_val_rmse: 0.5332\n","\n","16 steps took 6.84 seconds\n","Epoch: 1 batch_num: 3 val_rmse: 0.5567\n","Still best_val_rmse: 0.5332 (from epoch 0)\n","\n","16 steps took 6.5 seconds\n","Epoch: 1 batch_num: 19 val_rmse: 0.5297\n","New best_val_rmse: 0.5297\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 35 val_rmse: 0.4988\n","New best_val_rmse: 0.4988\n","\n","8 steps took 3.26 seconds\n","Epoch: 1 batch_num: 43 val_rmse: 0.4966\n","New best_val_rmse: 0.4966\n","\n","8 steps took 3.28 seconds\n","Epoch: 1 batch_num: 51 val_rmse: 0.4888\n","New best_val_rmse: 0.4888\n","\n","4 steps took 1.64 seconds\n","Epoch: 1 batch_num: 55 val_rmse: 0.5528\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 71 val_rmse: 0.5637\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","16 steps took 6.52 seconds\n","Epoch: 1 batch_num: 87 val_rmse: 0.4922\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","8 steps took 3.26 seconds\n","Epoch: 1 batch_num: 95 val_rmse: 0.5322\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 111 val_rmse: 0.5102\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 127 val_rmse: 0.4952\n","Still best_val_rmse: 0.4888 (from epoch 1)\n","\n","8 steps took 3.28 seconds\n","Epoch: 1 batch_num: 135 val_rmse: 0.4775\n","New best_val_rmse: 0.4775\n","\n","2 steps took 0.821 seconds\n","Epoch: 1 batch_num: 137 val_rmse: 0.4808\n","Still best_val_rmse: 0.4775 (from epoch 1)\n","\n","4 steps took 1.91 seconds\n","Epoch: 2 batch_num: 0 val_rmse: 0.5055\n","Still best_val_rmse: 0.4775 (from epoch 1)\n","\n","16 steps took 6.52 seconds\n","Epoch: 2 batch_num: 16 val_rmse: 0.4875\n","Still best_val_rmse: 0.4775 (from epoch 1)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 20 val_rmse: 0.4893\n","Still best_val_rmse: 0.4775 (from epoch 1)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 24 val_rmse: 0.4721\n","New best_val_rmse: 0.4721\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 26 val_rmse: 0.471\n","New best_val_rmse: 0.471\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 28 val_rmse: 0.4761\n","Still best_val_rmse: 0.471 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 30 val_rmse: 0.4797\n","Still best_val_rmse: 0.471 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 32 val_rmse: 0.4828\n","Still best_val_rmse: 0.471 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 36 val_rmse: 0.4725\n","Still best_val_rmse: 0.471 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 38 val_rmse: 0.4695\n","New best_val_rmse: 0.4695\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 39 val_rmse: 0.4695\n","Still best_val_rmse: 0.4695 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 40 val_rmse: 0.4686\n","New best_val_rmse: 0.4686\n","\n","1 steps took 0.408 seconds\n","Epoch: 2 batch_num: 41 val_rmse: 0.4679\n","New best_val_rmse: 0.4679\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 42 val_rmse: 0.4689\n","Still best_val_rmse: 0.4679 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 43 val_rmse: 0.4737\n","Still best_val_rmse: 0.4679 (from epoch 2)\n","\n","2 steps took 0.817 seconds\n","Epoch: 2 batch_num: 45 val_rmse: 0.4808\n","Still best_val_rmse: 0.4679 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 49 val_rmse: 0.4969\n","Still best_val_rmse: 0.4679 (from epoch 2)\n","\n","8 steps took 3.25 seconds\n","Epoch: 2 batch_num: 57 val_rmse: 0.4644\n","New best_val_rmse: 0.4644\n","\n","1 steps took 0.416 seconds\n","Epoch: 2 batch_num: 58 val_rmse: 0.4652\n","Still best_val_rmse: 0.4644 (from epoch 2)\n","\n","1 steps took 0.417 seconds\n","Epoch: 2 batch_num: 59 val_rmse: 0.4663\n","Still best_val_rmse: 0.4644 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 60 val_rmse: 0.4666\n","Still best_val_rmse: 0.4644 (from epoch 2)\n","\n","1 steps took 0.415 seconds\n","Epoch: 2 batch_num: 61 val_rmse: 0.4651\n","Still best_val_rmse: 0.4644 (from epoch 2)\n","\n","1 steps took 0.408 seconds\n","Epoch: 2 batch_num: 62 val_rmse: 0.4632\n","New best_val_rmse: 0.4632\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 63 val_rmse: 0.4626\n","New best_val_rmse: 0.4626\n","\n","1 steps took 0.416 seconds\n","Epoch: 2 batch_num: 64 val_rmse: 0.4627\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 65 val_rmse: 0.4644\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 66 val_rmse: 0.4675\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 67 val_rmse: 0.4708\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 69 val_rmse: 0.4788\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 71 val_rmse: 0.4802\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 75 val_rmse: 0.4692\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 76 val_rmse: 0.465\n","Still best_val_rmse: 0.4626 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 77 val_rmse: 0.4625\n","New best_val_rmse: 0.4625\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 78 val_rmse: 0.4606\n","New best_val_rmse: 0.4606\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 79 val_rmse: 0.4603\n","New best_val_rmse: 0.4603\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 80 val_rmse: 0.4607\n","Still best_val_rmse: 0.4603 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 81 val_rmse: 0.4611\n","Still best_val_rmse: 0.4603 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 82 val_rmse: 0.4612\n","Still best_val_rmse: 0.4603 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 83 val_rmse: 0.4609\n","Still best_val_rmse: 0.4603 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 84 val_rmse: 0.4604\n","Still best_val_rmse: 0.4603 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 85 val_rmse: 0.4601\n","New best_val_rmse: 0.4601\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 86 val_rmse: 0.46\n","New best_val_rmse: 0.46\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 87 val_rmse: 0.4603\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 88 val_rmse: 0.4608\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 89 val_rmse: 0.4614\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 90 val_rmse: 0.4619\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 91 val_rmse: 0.4621\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 92 val_rmse: 0.4623\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 93 val_rmse: 0.4622\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.416 seconds\n","Epoch: 2 batch_num: 94 val_rmse: 0.4623\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 95 val_rmse: 0.4625\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.415 seconds\n","Epoch: 2 batch_num: 96 val_rmse: 0.4628\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 97 val_rmse: 0.463\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 98 val_rmse: 0.4633\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 99 val_rmse: 0.4636\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 100 val_rmse: 0.4642\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.416 seconds\n","Epoch: 2 batch_num: 101 val_rmse: 0.4649\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 102 val_rmse: 0.466\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 103 val_rmse: 0.4667\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 104 val_rmse: 0.4674\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 105 val_rmse: 0.4676\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 106 val_rmse: 0.4673\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 107 val_rmse: 0.4668\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 108 val_rmse: 0.4662\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 109 val_rmse: 0.4656\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 110 val_rmse: 0.465\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 111 val_rmse: 0.4644\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 112 val_rmse: 0.4639\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 113 val_rmse: 0.4636\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 114 val_rmse: 0.4632\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 115 val_rmse: 0.4629\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 116 val_rmse: 0.4625\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 117 val_rmse: 0.4623\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.414 seconds\n","Epoch: 2 batch_num: 118 val_rmse: 0.4622\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 119 val_rmse: 0.4621\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 120 val_rmse: 0.462\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 121 val_rmse: 0.4619\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 122 val_rmse: 0.4619\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 123 val_rmse: 0.4619\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 124 val_rmse: 0.4618\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.409 seconds\n","Epoch: 2 batch_num: 125 val_rmse: 0.4618\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 126 val_rmse: 0.4617\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 127 val_rmse: 0.4616\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 128 val_rmse: 0.4616\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.418 seconds\n","Epoch: 2 batch_num: 129 val_rmse: 0.4616\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 130 val_rmse: 0.4616\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 131 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 132 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.411 seconds\n","Epoch: 2 batch_num: 133 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.41 seconds\n","Epoch: 2 batch_num: 134 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 135 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 136 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 137 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.412 seconds\n","Epoch: 2 batch_num: 138 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 139 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","1 steps took 0.413 seconds\n","Epoch: 2 batch_num: 140 val_rmse: 0.4615\n","Still best_val_rmse: 0.46 (from epoch 2)\n","\n","Performance estimates:\n","[0.4820976694639964, 0.4600287815619605]\n","Mean: 0.4710632255129784\n","\n","Fold 3/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 7.21 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 1.22\n","New best_val_rmse: 1.22\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.9518\n","New best_val_rmse: 0.9518\n","\n","16 steps took 6.56 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.7003\n","New best_val_rmse: 0.7003\n","\n","16 steps took 6.56 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.6381\n","New best_val_rmse: 0.6381\n","\n","16 steps took 6.56 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.6306\n","New best_val_rmse: 0.6306\n","\n","16 steps took 6.56 seconds\n","Epoch: 0 batch_num: 96 val_rmse: 0.6332\n","Still best_val_rmse: 0.6306 (from epoch 0)\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 112 val_rmse: 0.5775\n","New best_val_rmse: 0.5775\n","\n","16 steps took 6.57 seconds\n","Epoch: 0 batch_num: 128 val_rmse: 0.5401\n","New best_val_rmse: 0.5401\n","\n","16 steps took 6.88 seconds\n","Epoch: 1 batch_num: 3 val_rmse: 0.5843\n","Still best_val_rmse: 0.5401 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 19 val_rmse: 0.5249\n","New best_val_rmse: 0.5249\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 35 val_rmse: 0.5735\n","Still best_val_rmse: 0.5249 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 51 val_rmse: 0.5054\n","New best_val_rmse: 0.5054\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 67 val_rmse: 0.5037\n","New best_val_rmse: 0.5037\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 83 val_rmse: 0.4898\n","New best_val_rmse: 0.4898\n","\n","4 steps took 1.63 seconds\n","Epoch: 1 batch_num: 87 val_rmse: 0.5653\n","Still best_val_rmse: 0.4898 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 103 val_rmse: 0.4978\n","Still best_val_rmse: 0.4898 (from epoch 1)\n","\n","8 steps took 3.29 seconds\n","Epoch: 1 batch_num: 111 val_rmse: 0.5553\n","Still best_val_rmse: 0.4898 (from epoch 1)\n","\n","16 steps took 6.56 seconds\n","Epoch: 1 batch_num: 127 val_rmse: 0.5385\n","Still best_val_rmse: 0.4898 (from epoch 1)\n","\n","16 steps took 6.91 seconds\n","Epoch: 2 batch_num: 2 val_rmse: 0.5415\n","Still best_val_rmse: 0.4898 (from epoch 1)\n","\n","16 steps took 6.57 seconds\n","Epoch: 2 batch_num: 18 val_rmse: 0.4893\n","New best_val_rmse: 0.4893\n","\n","4 steps took 1.65 seconds\n","Epoch: 2 batch_num: 22 val_rmse: 0.5001\n","Still best_val_rmse: 0.4893 (from epoch 2)\n","\n","16 steps took 6.57 seconds\n","Epoch: 2 batch_num: 38 val_rmse: 0.4841\n","New best_val_rmse: 0.4841\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 42 val_rmse: 0.487\n","Still best_val_rmse: 0.4841 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 46 val_rmse: 0.4741\n","New best_val_rmse: 0.4741\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 48 val_rmse: 0.473\n","New best_val_rmse: 0.473\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 50 val_rmse: 0.4726\n","New best_val_rmse: 0.4726\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 52 val_rmse: 0.4725\n","New best_val_rmse: 0.4725\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 54 val_rmse: 0.4728\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 56 val_rmse: 0.473\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 58 val_rmse: 0.4751\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.824 seconds\n","Epoch: 2 batch_num: 60 val_rmse: 0.4755\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 62 val_rmse: 0.4751\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 64 val_rmse: 0.4732\n","Still best_val_rmse: 0.4725 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 66 val_rmse: 0.4721\n","New best_val_rmse: 0.4721\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 68 val_rmse: 0.4713\n","New best_val_rmse: 0.4713\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 70 val_rmse: 0.4712\n","New best_val_rmse: 0.4712\n","\n","2 steps took 0.817 seconds\n","Epoch: 2 batch_num: 72 val_rmse: 0.4711\n","New best_val_rmse: 0.4711\n","\n","2 steps took 0.816 seconds\n","Epoch: 2 batch_num: 74 val_rmse: 0.4711\n","New best_val_rmse: 0.4711\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 76 val_rmse: 0.4711\n","New best_val_rmse: 0.4711\n","\n","2 steps took 0.824 seconds\n","Epoch: 2 batch_num: 78 val_rmse: 0.4711\n","New best_val_rmse: 0.4711\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 80 val_rmse: 0.4712\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 82 val_rmse: 0.4715\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 84 val_rmse: 0.4717\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 86 val_rmse: 0.4724\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.825 seconds\n","Epoch: 2 batch_num: 88 val_rmse: 0.474\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 90 val_rmse: 0.4753\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 92 val_rmse: 0.4758\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 94 val_rmse: 0.475\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 96 val_rmse: 0.4742\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 98 val_rmse: 0.4732\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 100 val_rmse: 0.4725\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.816 seconds\n","Epoch: 2 batch_num: 102 val_rmse: 0.472\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 104 val_rmse: 0.4718\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.825 seconds\n","Epoch: 2 batch_num: 106 val_rmse: 0.4716\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.823 seconds\n","Epoch: 2 batch_num: 108 val_rmse: 0.4716\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 110 val_rmse: 0.4714\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 112 val_rmse: 0.4712\n","Still best_val_rmse: 0.4711 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 114 val_rmse: 0.471\n","New best_val_rmse: 0.471\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 116 val_rmse: 0.4709\n","New best_val_rmse: 0.4709\n","\n","2 steps took 0.817 seconds\n","Epoch: 2 batch_num: 118 val_rmse: 0.4708\n","New best_val_rmse: 0.4708\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 120 val_rmse: 0.4707\n","New best_val_rmse: 0.4707\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 122 val_rmse: 0.4707\n","New best_val_rmse: 0.4707\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 124 val_rmse: 0.4707\n","New best_val_rmse: 0.4707\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 126 val_rmse: 0.4707\n","New best_val_rmse: 0.4707\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 128 val_rmse: 0.4706\n","New best_val_rmse: 0.4706\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 130 val_rmse: 0.4706\n","New best_val_rmse: 0.4706\n","\n","2 steps took 0.824 seconds\n","Epoch: 2 batch_num: 132 val_rmse: 0.4706\n","New best_val_rmse: 0.4706\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 134 val_rmse: 0.4706\n","Still best_val_rmse: 0.4706 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 136 val_rmse: 0.4706\n","Still best_val_rmse: 0.4706 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 138 val_rmse: 0.4706\n","New best_val_rmse: 0.4706\n","\n","2 steps took 0.823 seconds\n","Epoch: 2 batch_num: 140 val_rmse: 0.4706\n","New best_val_rmse: 0.4706\n","\n","Performance estimates:\n","[0.4820976694639964, 0.4600287815619605, 0.47056723921714677]\n","Mean: 0.47089789674770116\n","\n","Fold 4/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 7.21 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 0.9139\n","New best_val_rmse: 0.9139\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.6786\n","New best_val_rmse: 0.6786\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.6368\n","New best_val_rmse: 0.6368\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.5899\n","New best_val_rmse: 0.5899\n","\n","16 steps took 6.52 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.5999\n","Still best_val_rmse: 0.5899 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 96 val_rmse: 0.6035\n","Still best_val_rmse: 0.5899 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 112 val_rmse: 0.6227\n","Still best_val_rmse: 0.5899 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 128 val_rmse: 0.5555\n","New best_val_rmse: 0.5555\n","\n","16 steps took 6.85 seconds\n","Epoch: 1 batch_num: 3 val_rmse: 0.5431\n","New best_val_rmse: 0.5431\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 19 val_rmse: 0.5287\n","New best_val_rmse: 0.5287\n","\n","16 steps took 6.52 seconds\n","Epoch: 1 batch_num: 35 val_rmse: 0.4948\n","New best_val_rmse: 0.4948\n","\n","8 steps took 3.27 seconds\n","Epoch: 1 batch_num: 43 val_rmse: 0.5335\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 59 val_rmse: 0.5396\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 75 val_rmse: 0.5045\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 91 val_rmse: 0.5123\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 107 val_rmse: 0.4965\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","8 steps took 3.28 seconds\n","Epoch: 1 batch_num: 115 val_rmse: 0.5032\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 131 val_rmse: 0.5406\n","Still best_val_rmse: 0.4948 (from epoch 1)\n","\n","16 steps took 6.82 seconds\n","Epoch: 2 batch_num: 6 val_rmse: 0.4927\n","New best_val_rmse: 0.4927\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 14 val_rmse: 0.5019\n","Still best_val_rmse: 0.4927 (from epoch 2)\n","\n","16 steps took 6.53 seconds\n","Epoch: 2 batch_num: 30 val_rmse: 0.4942\n","Still best_val_rmse: 0.4927 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 38 val_rmse: 0.4899\n","New best_val_rmse: 0.4899\n","\n","4 steps took 1.65 seconds\n","Epoch: 2 batch_num: 42 val_rmse: 0.4911\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 50 val_rmse: 0.4929\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.28 seconds\n","Epoch: 2 batch_num: 58 val_rmse: 0.4956\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.26 seconds\n","Epoch: 2 batch_num: 66 val_rmse: 0.4908\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.28 seconds\n","Epoch: 2 batch_num: 74 val_rmse: 0.5009\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","16 steps took 6.55 seconds\n","Epoch: 2 batch_num: 90 val_rmse: 0.4943\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.28 seconds\n","Epoch: 2 batch_num: 98 val_rmse: 0.4919\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 106 val_rmse: 0.4913\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 114 val_rmse: 0.4933\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 122 val_rmse: 0.4946\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 130 val_rmse: 0.4949\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","8 steps took 3.27 seconds\n","Epoch: 2 batch_num: 138 val_rmse: 0.4949\n","Still best_val_rmse: 0.4899 (from epoch 2)\n","\n","Performance estimates:\n","[0.4820976694639964, 0.4600287815619605, 0.47056723921714677, 0.489873864959141]\n","Mean: 0.47564188880056113\n","\n","Fold 5/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 7.22 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 0.9178\n","New best_val_rmse: 0.9178\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.7791\n","New best_val_rmse: 0.7791\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.6109\n","New best_val_rmse: 0.6109\n","\n","16 steps took 6.55 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.7195\n","Still best_val_rmse: 0.6109 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.5772\n","New best_val_rmse: 0.5772\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 96 val_rmse: 0.5917\n","Still best_val_rmse: 0.5772 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 112 val_rmse: 0.6578\n","Still best_val_rmse: 0.5772 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 0 batch_num: 128 val_rmse: 0.5138\n","New best_val_rmse: 0.5138\n","\n","16 steps took 6.88 seconds\n","Epoch: 1 batch_num: 3 val_rmse: 0.5219\n","Still best_val_rmse: 0.5138 (from epoch 0)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 19 val_rmse: 0.5111\n","New best_val_rmse: 0.5111\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 35 val_rmse: 0.5148\n","Still best_val_rmse: 0.5111 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 51 val_rmse: 0.5233\n","Still best_val_rmse: 0.5111 (from epoch 1)\n","\n","16 steps took 6.53 seconds\n","Epoch: 1 batch_num: 67 val_rmse: 0.4881\n","New best_val_rmse: 0.4881\n","\n","4 steps took 1.64 seconds\n","Epoch: 1 batch_num: 71 val_rmse: 0.4872\n","New best_val_rmse: 0.4872\n","\n","4 steps took 1.63 seconds\n","Epoch: 1 batch_num: 75 val_rmse: 0.5036\n","Still best_val_rmse: 0.4872 (from epoch 1)\n","\n","16 steps took 6.54 seconds\n","Epoch: 1 batch_num: 91 val_rmse: 0.5013\n","Still best_val_rmse: 0.4872 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 107 val_rmse: 0.4861\n","New best_val_rmse: 0.4861\n","\n","4 steps took 1.65 seconds\n","Epoch: 1 batch_num: 111 val_rmse: 0.5129\n","Still best_val_rmse: 0.4861 (from epoch 1)\n","\n","16 steps took 6.55 seconds\n","Epoch: 1 batch_num: 127 val_rmse: 0.4939\n","Still best_val_rmse: 0.4861 (from epoch 1)\n","\n","8 steps took 3.27 seconds\n","Epoch: 1 batch_num: 135 val_rmse: 0.484\n","New best_val_rmse: 0.484\n","\n","4 steps took 1.63 seconds\n","Epoch: 1 batch_num: 139 val_rmse: 0.4926\n","Still best_val_rmse: 0.484 (from epoch 1)\n","\n","8 steps took 3.58 seconds\n","Epoch: 2 batch_num: 6 val_rmse: 0.4863\n","Still best_val_rmse: 0.484 (from epoch 1)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 10 val_rmse: 0.4832\n","New best_val_rmse: 0.4832\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 14 val_rmse: 0.4927\n","Still best_val_rmse: 0.4832 (from epoch 2)\n","\n","8 steps took 3.28 seconds\n","Epoch: 2 batch_num: 22 val_rmse: 0.4786\n","New best_val_rmse: 0.4786\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 24 val_rmse: 0.4817\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 28 val_rmse: 0.4827\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 32 val_rmse: 0.4824\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 36 val_rmse: 0.5031\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","16 steps took 6.54 seconds\n","Epoch: 2 batch_num: 52 val_rmse: 0.4824\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 56 val_rmse: 0.48\n","Still best_val_rmse: 0.4786 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 60 val_rmse: 0.4771\n","New best_val_rmse: 0.4771\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 62 val_rmse: 0.4783\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 64 val_rmse: 0.4788\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 66 val_rmse: 0.4789\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 68 val_rmse: 0.4783\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 70 val_rmse: 0.478\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 72 val_rmse: 0.4785\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 74 val_rmse: 0.4811\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 78 val_rmse: 0.4855\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","4 steps took 1.63 seconds\n","Epoch: 2 batch_num: 82 val_rmse: 0.4867\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","4 steps took 1.65 seconds\n","Epoch: 2 batch_num: 86 val_rmse: 0.48\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","4 steps took 1.64 seconds\n","Epoch: 2 batch_num: 90 val_rmse: 0.4772\n","Still best_val_rmse: 0.4771 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 92 val_rmse: 0.476\n","New best_val_rmse: 0.476\n","\n","2 steps took 0.824 seconds\n","Epoch: 2 batch_num: 94 val_rmse: 0.4751\n","New best_val_rmse: 0.4751\n","\n","2 steps took 0.823 seconds\n","Epoch: 2 batch_num: 96 val_rmse: 0.4746\n","New best_val_rmse: 0.4746\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 98 val_rmse: 0.4743\n","New best_val_rmse: 0.4743\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 100 val_rmse: 0.474\n","New best_val_rmse: 0.474\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 102 val_rmse: 0.4737\n","New best_val_rmse: 0.4737\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 104 val_rmse: 0.4734\n","New best_val_rmse: 0.4734\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 106 val_rmse: 0.4733\n","New best_val_rmse: 0.4733\n","\n","2 steps took 0.817 seconds\n","Epoch: 2 batch_num: 108 val_rmse: 0.4732\n","New best_val_rmse: 0.4732\n","\n","2 steps took 0.823 seconds\n","Epoch: 2 batch_num: 110 val_rmse: 0.4731\n","New best_val_rmse: 0.4731\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 112 val_rmse: 0.4732\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 114 val_rmse: 0.4735\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.823 seconds\n","Epoch: 2 batch_num: 116 val_rmse: 0.4737\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 118 val_rmse: 0.474\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.822 seconds\n","Epoch: 2 batch_num: 120 val_rmse: 0.4742\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.826 seconds\n","Epoch: 2 batch_num: 122 val_rmse: 0.4742\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 124 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 126 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 128 val_rmse: 0.4744\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.821 seconds\n","Epoch: 2 batch_num: 130 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.824 seconds\n","Epoch: 2 batch_num: 132 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 134 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.819 seconds\n","Epoch: 2 batch_num: 136 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.82 seconds\n","Epoch: 2 batch_num: 138 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","2 steps took 0.818 seconds\n","Epoch: 2 batch_num: 140 val_rmse: 0.4743\n","Still best_val_rmse: 0.4731 (from epoch 2)\n","\n","Performance estimates:\n","[0.4820976694639964, 0.4600287815619605, 0.47056723921714677, 0.489873864959141, 0.47310312354348444]\n","Mean: 0.47513413574914576\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u3Kzic8xf6xY"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"nx2p0qlbp24J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627138823266,"user_tz":240,"elapsed":40572,"user":{"displayName":"Yunlin Qi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-aKZVJ1prt3w6IZCyj5ulz60dvuBwEfir7h5F=s64","userId":"04948370213367288353"}},"outputId":"b0894b2c-f5f0-41b7-b773-c5a36eda3889"},"source":["test_dataset = LitDataset(test_df, inference_only=True)\n","all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n","\n","test_dataset = LitDataset(test_df, inference_only=True)\n","test_loader = DataLoader(test_dataset, batch_size=dataset_config[\"batch_size\"],\n","                         drop_last=False, shuffle=False, num_workers=dataset_config[\"num_workers\"])\n","\n","for index in range(len(list_val_rmse)):            \n","    model_path = f\"model_{index + 1}.pth\"\n","    print(f\"\\nUsing {model_path}\")\n","                        \n","    model = LitModel(model_config[\"base_model\"])\n","    model.load_state_dict(torch.load(model_path))    \n","    model.to(DEVICE)\n","    \n","    all_predictions[index] = predict(model, test_loader)\n","    \n","    del model\n","    gc.collect()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Using model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"prdKGtXmqUi7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627138823267,"user_tz":240,"elapsed":28,"user":{"displayName":"Yunlin Qi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-aKZVJ1prt3w6IZCyj5ulz60dvuBwEfir7h5F=s64","userId":"04948370213367288353"}},"outputId":"b73a1074-ea2c-410f-e80a-b0ccdab05266"},"source":["# Generate Submission\n","#_, _, submission_df = get_data()\n","submission_df = sample_submission\n","predictions = all_predictions.mean(axis=0)\n","submission_df.target = predictions\n","print(submission_df)\n","submission_df.to_csv(evaluation_config[\"submission_path\"], index=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          id    target\n","0  c0f722661 -0.525537\n","1  f0953f0a5 -0.493696\n","2  0df072751 -0.341860\n","3  04caf4e0c -2.563208\n","4  0e63f8bea -1.846457\n","5  12537fe78 -1.380682\n","6  965e592c0  0.255189\n"],"name":"stdout"}]}]}